






































































































































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 264/265 [05:33<00:01,  1.24s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 265/265 [05:35<00:00,  1.26s/it]
>>>>>>>>> Validation #1  Accuracy: 63.92%  Average Loss: 0.1668 f1 score: 0.5138
>>>>>>>>> Best performance at epoch: 1
>>>>>>>>> Save model in /opt/ml/baseline/experiments/efficientnet_b3_pruned_multihead3/batch64_focalloss_imgshape300_ReduceLROnPlateau_G








  5%|██████████▉                                                                                                                                                                                                                    | 13/265 [00:19<06:08,  1.46s/it]
Traceback (most recent call last):
  File "train.py", line 102, in <module>
    main(config)
  File "train.py", line 90, in main
    trainer.train()
  File "/opt/ml/baseline/trainer/trainer.py", line 206, in train
    self.optimizer.step()
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py", line 108, in step
    F.adam(params_with_grad,
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py", line 87, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt
Traceback (most recent call last):
  File "train.py", line 102, in <module>
    main(config)
  File "train.py", line 90, in main
    trainer.train()
  File "/opt/ml/baseline/trainer/trainer.py", line 206, in train
    self.optimizer.step()
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py", line 108, in step
    F.adam(params_with_grad,
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py", line 87, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt