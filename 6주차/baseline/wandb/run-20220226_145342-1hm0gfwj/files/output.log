




















































































































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 470/472 [04:56<00:01,  1.47it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 472/472 [04:57<00:00,  1.59it/s]
>>>>>>>>> Validation #1  Accuracy: 88.54%  Average Loss: 0.9472 f1 score: 0.5092
>>>>>>>>> Best performance at epoch: 1
>>>>>>>>> Save model in /opt/ml/baseline/experiments/efficientnet_b3_pruned2/batch32_focalloss_imgshape300_ReduceLROnPlateau







  6%|███████▉                                                                                                                                   | 27/472 [00:18<04:56,  1.50it/s]
Traceback (most recent call last):
  File "train.py", line 128, in <module>
    main(config)
  File "train.py", line 116, in main
    trainer.train()
  File "/opt/ml/baseline/trainer/trainer.py", line 58, in train
    self.optimizer.step()
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py", line 108, in step
    F.adam(params_with_grad,
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py", line 87, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt
Traceback (most recent call last):
  File "train.py", line 128, in <module>
    main(config)
  File "train.py", line 116, in main
    trainer.train()
  File "/opt/ml/baseline/trainer/trainer.py", line 58, in train
    self.optimizer.step()
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py", line 108, in step
    F.adam(params_with_grad,
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py", line 87, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt