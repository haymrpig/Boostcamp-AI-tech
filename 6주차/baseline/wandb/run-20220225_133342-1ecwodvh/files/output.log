













































































































































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 235/236 [05:47<00:01,  1.52s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236/236 [05:49<00:00,  1.48s/it]
>>>>>>>>> Validation #1  Accuracy: 92.93%  Average Loss: 0.2086
>>>>>>>>> Best performance at epoch: 1
>>>>>>>>> Save model in /opt/ml/baseline/experiments/exp8/efficientnet_b0











































































































 61%|██████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 145/236 [03:36<02:15,  1.49s/it]
Traceback (most recent call last):
  File "train.py", line 125, in <module>
    main(config)
  File "train.py", line 113, in main
    trainer.train()
  File "/opt/ml/baseline/trainer/trainer.py", line 56, in train
    self.optimizer.step()
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py", line 108, in step
    F.adam(params_with_grad,
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py", line 86, in adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt