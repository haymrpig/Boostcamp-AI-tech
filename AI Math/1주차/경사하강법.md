# 목차

1. **경사하강법**

   - 미분

   - 경사하강/상승법

     - 확률적 경사하강법 (stochastic gradient descent)

   - 선형회귀 계수 구하기

     

# 1. 경사하강법

- **미분**

  변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구

  2차 포물선을 예로 들면 미분이란 **x점에서의 접선의 기울기**를 의미한다. 즉, **경사하강/경사상승법**에서는 접선의 기울기를 통해 함수값을 증가시켜야 하는지 감소시켜야 하는지 알 수 있다. 

  ( 증가시키고 싶을 때 : +미분값, 감소시키고 싶을 때 : -미분값, 0일때는 업데이트 X )
  $$
  f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}
  $$

  ```python
  import sympy as sym
  from sympy.abc import x
  
  sym.diff(sym.poly(x**2+x),x)	# x에 대해 미분
  # poly : 다항함수
  # cos, sin 등 다양한 함수 가능
  ```

  

- **경사하강/상승법**

  미분값을 통해 함수값을 증가/감소시키며, 만약 다변수 함수의 경우 **편미분** (partial differentiation)을 사용한다. 

  실제 코드에서는 미분값이 0이 되는 경우는 거의 없기 때문에, limit를 정한다. 

  아래 식은 그레디언트 (gradient) 벡터를 의미한다. 
  $$
  \nabla f = (\partial_{x_1}f,\partial_{x_2}f,...,\partial_{x_d}f ), (\nabla:nabla)
  $$
  L2 노름을 사용하는 경우 목적식이 볼록함수가 되기 때문에 항상 수렴하지만, **비선형 모델**의 경우 항상 볼록하지 않기 때문에 **수렴이 보장되지 않는다**!! 

  - **확률적 경사하강법 (stochastic gradient descent, minibatch)**

    모든 데이터를 사용하는 것이 아닌 데이터의 일부를 사용하여 업데이트 하는 것=minibatch

    보통 SGD라고 하면 minibatch gradient descent를 의미한다. 

    - **장점**

      1. 모든 데이터를 다 쓰지 않기 때문에 연산자원을 좀 더 효율적으로 활용할 수 있다. 하드웨어에 부담이 적으며, 학습시간이 단축된다. 

      2. GD와 매우 유사하며, 이 둘의 기댓값은 확률적으로 비슷하다고 보장이 되기 때문에 GD처럼 사용할 수 있다.  

      3.  비선형의 경우 GD는 볼록함수가 되지 않아 최소점을 찾는다는 보장이 없지만, SGD의 경우 일부 데이터만을 사용하기 때문에 매 step마다의 극소점이 다르게 되며, 목적식이 달라짐에 따라서 극소점이 아닌 경우를 탈출할 수 있게 된다.  

         즉, 볼록이 아닌 목적식에도 사용이 가능하다. 

      

- **선형회귀 계수 구하기**

  무어펜로즈 역행렬을 이용하여 선형회귀 계수를 찾아도 되지만, 이 경우는 선형일 때만 가능하기에 일반적으로 **경사하강법**을 이용하여 선형회귀 계수를 찾는다. 

  선형회귀의 목적식은
  $$
  ||y-X\beta||_2이고,\; 무어펜로즈\; 역행렬이\; 아닌\; 이를\; 최소화하는\; \beta를 \;찾아야\; 함으로,\\
  미분, \;즉,\; 그레디언트\; 벡터를\; 구해야\; 한다.\\
  \beta에\; 대한\; 최소값을\; 찾는\; 것이기\; 때문에\; \beta로 \;미분한다.\\\\
  \nabla_\beta||y-X\beta||_2=(\partial_{\beta_1}||y-X\beta||_2, ..., \partial_{\beta_d}||y-X\beta||_2)\\
  \partial_{\beta_k}||y-X\beta||_2=\partial_{\beta_k}\{\frac{1}{n}\sum_{i=1}^n(y_i-\sum_{j=1}^d X_{ij}\beta_j)^2\}^{\frac{1}{2}}\\
  (이전의\; 보았던\; L2\; 노름과\; 식이\; 좀\; 다른\; 이유는 \\n개의\; 평균을\; 계산해주기\; 위해서\; n으로\; 나눴기\; 때문이다. )\\\\
  위\; 식을\; 풀이하면 \;-\frac{X_{.k}^T(y-X\beta)}{n||y-X\beta||_2}가 나온다.\;(X_{.k}는\; k번째\; 열) \\
  즉,\; 전체\; 행렬에\; 대해서\; 편미분한\; 결과는\; -\frac{X^T(y-X\beta)}{n||y-X\beta||_2}가\; 된다.
  $$
  여기서 경사하강법 식은
  $$
  \beta^{t+1}=\beta^t-learning\_rate*\nabla_\beta||y-X\beta^t||_2
  $$
  이 된다. 

  전체 행렬에 대한 편미분 식은 sqrt를 씌우지 않고 계산해도 똑같은 결과가 나오고, 계산 결과가 더 깔끔하기 때문에 두 방법 모두 사용해도 된다. 

  