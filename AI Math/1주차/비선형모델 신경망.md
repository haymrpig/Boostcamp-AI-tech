# 1. 비선형모델 신경망 (neural network)

선형모델의 목적은 error를 최소화하는 weight와 bias를 찾는 것이다. 

수식으로 나타내면 아래와 같다. ( beta = parameter )  
![image](https://user-images.githubusercontent.com/71866756/150138505-14615c40-b1c4-47e2-957e-35698e7d9aba.png)  


- **소프트맥스 함수 (softmax)**

  **분류 문제**를 풀 때, 선형모델과 소프트맥스 함수를 결합하여 예측한다. 

  소프트맥스 함수는 0~1 사이 **확률값**으로 나오기 때문에 분류 문제에 적합하다.   
  ![image](https://user-images.githubusercontent.com/71866756/150138552-6ca53f33-2130-4470-9bac-06a37874c1e8.png)  
  위 식이 softmax함수이다. 

  만약 분류 문제라고 한다면, output인 o는 nxp shape의 matrix이면 여기서 p는 class의 개수, n은 데이터의 개수라고 생각하면 된다. 

  즉, 분자인 exp(o1)은 1번 데이터가 p개의 클래스에 속할 비중에 exponential을 취한 것이고, 분모는 전체 p개의 클래스에 속할 비중들의 합을 구한 것이다. 

  여기서 나눠주는 이유는 확률의 합은 1이므로 정규화를 시켜준 것이라고 이해할 수 있다. 

   

  ```python
  def softmax(vec):
  	denumerator = np.exp(vec - np.max(vec, axis=-1, keepdim=True))
      # np.max는 값이 너무 커지는 것을 방지한다. 
  	numerator = np.sum(denumerator, axis=-1, keepdim=True)
  	val = denumerator / numerator
  	return val
  ```

  
  
- **one_hot encoding**

  원핫은 벡터의 최대값을 가진 주소만 1로 출력한다. 

  학습이 아닌 **추론 문제**에 적합하며, softmax함수를 사용하지 않는다. 





  
- **활성함수 (activation function)**

  활성함수는 비선형 함수로써 출력에 나오는 각각의 값에 개별적으로 적용이 된다. 

  softmax와의 차이점은 softmax는 모든 값을 통틀어서 확률로 변환을 했다고 이해하면, 

  활성함수는 단순히 **각각의 값**이 비선형 함수를 거쳐 선형을 **비선형 (잠재벡터, 히든벡터)**으로 모델링하는 것이다. 

  (따라서, 활성함수는 vector를 input으로 받는게 아니라, 실수값을 input으로 받는다.)

  
  - **시그모이드 (sigmoid)**  
    ![image](https://user-images.githubusercontent.com/71866756/150138608-aeb89dfe-998a-4e80-a3b4-3e358dfa4692.png)  
      
  - **하이퍼볼릭 탄젠트 (hyperbolic tangent)**  
    ![image](https://user-images.githubusercontent.com/71866756/150138669-371fc1af-2981-44f4-bba1-989550f27469.png)  
      
  - **ReLU**   
  ![image](https://user-images.githubusercontent.com/71866756/150138717-bcc675b1-8095-4bea-817b-43d98d99d7ad.png)  
  
  - 용어정리

    `잠재벡터, 히든벡터, 뉴론` : 비선형 함수인 활성함수를 거쳐서 나온 output 벡터
    
    `퍼셉트론` :  선형모델에 활성함수를 씌운 기본적인 네트워크
    
    `2층 신경망` : input->활성함수->output으로 구성된 네트워크, 여기서 2 layer라고 불리는 이유는 가중치가 2개이기 때문이다. 
    
    ( input -> middle-output으로 가는 가중치1, 활성함수 -> output으로 가는 가중치2)
    
    `다층 신경망 (multi-layer perceptron)` : 여러개의 2층 신경망으로 구성된 네트워크


  
- **multi layer로 모델을 구성하는 이유**

  1. 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런의 숫자가 훨씬 빨리 줄어든다. 
  2. 층이 얇을수록 뉴런의 숫자가 기하급수적으로 늘어나 더 넓은 신경망이 된다. 
  
- **순전파 (forward propagation) & 역전파 (backpropagation)**

  `forward propagation` : 입력이 구성된 layer들을 따라 출력으로 나오는 과정 (**학습이 아니다.**)

  `backpropagation` : 출력부터 입력순으로 gradient를 계산하는 방식 (**학습하는 과정**), 현재 layer는 다음 layer의 gradient를 필요로 하며, **연쇄법칙 (chain rule) 기반 자동미분 (auto-differentiation)**에 의해 계산된다. 

  - **연쇄법칙**

    예시 )  
    ![image](https://user-images.githubusercontent.com/71866756/150138771-19019505-a00a-4a38-868c-dad1a8118c3a.png)  
    연쇄법칙을 계산하기 위해서는 각 노드의 **텐서값을 컴퓨터가 기억**해야 미분 계산이 가능하다. 따라서 순전파보다 역전파가 좀 더 **많은 메모리를 사용**한다. 

  
