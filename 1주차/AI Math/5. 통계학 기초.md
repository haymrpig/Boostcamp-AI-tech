# 1. 통계학

- **통계적 모델링**

  적절한 가정 위에서 확률분포를 추정하는 것이 목표!

  유한한 개수의 데이터를 관찰해서 모집단의 분포를 정확하게 아는 것은 불가능하므로 **근사적으로 추정**한다. 

  (근사적 추정으로 충분한 이유는 **예측모형의 목적**은 분포를 정확하게 맞추는 것이 아닌 데이터와 추정 방법의 불확실성을 고려해서 **위험을 최소화**하는 것이기 때문!!)

- **모수적 방법론 & 비모수적 방법론**

  - **모수적 방법론 (parametric)**

    데이터가 **특정 확률분포**를 따른다고 **선험적 (a priori)으로 가정**한 후, 그 분포를 결정하는 모수(parameter)를 추정하는 방법

  - **비모수 방법론 (nonparametric)**

    특정 **확률분포를 가정하지 않고**, 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌는 것  (기계학습의 대부분은 비모수 방법론!!)

    !!주의!!

    비모수 방법론이 모수가 없는 것이 아니라, 모수가 무한히 많거나 데이터에 따라 바뀌는 것

    비모수 방법론은 모수를 사용하지 않는다 -> (X)

  - **결론**

    모수와 비모수 방법론의 **차이점은 특정 확률분포를 가정하냐 아니냐**이다. 

- **확률분포 가정하는 방법**

  1. **히스토그램**을 통해 모양을 관찰한다. 
     - `베르누이 분포` : 데이터가 2개의 값 (0 or 1)만 가지는 경우
     - `카테고리 분포` : 데이터가 n개의 이산적인 값을 가지는 경우
     - `베타 분포` : 데이터가 [0,1] 사이에서 값을 가지는 경우
     - `감마 분포 / 로그정규 분포` : 데이터가 0 이상의 값을 가지는 경우
     - `정규분포 / 라프라스분포` : 데이터가 R (실수) 전체에서 값을 가지는 경우
  2. 분포마다 검정하는 방법들이 있으므로, 모수를 추정한 후에는 반드시 **검정**을 해야 한다. 

**EX) 정규분포의 모수는 평균과 분산으로, 이를 추정하는 통계량 (statistic)은 아래 예시와 같다.**  
![image](https://user-images.githubusercontent.com/71866756/150474012-87c01821-eacd-4158-bfd5-edb23b0d8bec.png)  

위 식에서 S^2을 N이 아닌 N-1로 나눠주는 이유는 표본분산을 구할 때, 기댓값을 취했을 때, 원래 모집단의 통계치와 일치하는 것을 유도하기 위함이다.  (**unbiased 추정량**을 구하기 위함)

> 사실 평균이나 분산을 구할 때 전체 개수로 나눠주는 게 아니라, 정확히 말하면 자유도의 개수로 나눠주는 것이다. 
>
> 여기서 자유도란 쉽게 말해서 해당 값을 정할 때 자유롭게 정하냐를 의미한다. 
>
> 예를 들어, 피자가 5조각 있고 5명이서 나눠가진다고 했을 때, 앞의 4명이 4개의 조각을 고르면 나머지 한명은 알아서 1조각이 결정이 된다. 이 때 자유도는 5-1 = 4라고 할 수 있다. 
>
> 평균을 구할 때는 전체 데이터의 값을 알아야 하기 때문에 자유도는 데이터의 개수인 n이다. 
>
> 하지만 평균을 구하고 나서 n개의 값 중 n-1개의 값을 알 때, 평균을 이용해서 나머지 1개의 값을 알 수 있으므로 자유도는 n-1이 된다. 
>
> 이처럼 표본 분산을 구할 때 평균을 알고 있기 때문에 나머지 하나의 값은 알고있는 것과 마찬가지이므로 자유도가 n-1이 된다. 
>
> 그래서 표본분산을 구할 때는 n-1로 나눠야 비편향 추정량을 구할 수가 있다. 
>
> (자세한 수식을 통한 증명은 이 [링크](https://www.youtube.com/watch?v=CLrUbG4ASQo)를 참고하자. )

- **표집분포 (sampling distribution)**

  통계량의 확률분포를 의미한다. 즉, 표본평균과 표본분산의 분포를 표집분포라고 한다. 

  **(!!!!!표분들의 분포 (sample distribution)와는 다르다!!!!!)**

  - **중심극한정리**  
    ![image](https://user-images.githubusercontent.com/71866756/150474098-43ac237c-55b1-4599-8152-7f038f419322.png)  
    즉, 식에서 볼 수 있듯이 표본평균의 분포는 N이 커질수록 분산이 작아지게 되어 점점 뾰족한 그래프가 된다. 

- **가능도 (likelihood)**

  데이터들이 주어지고, 이러한 데이터들은 어떠한 확률 분포로 존재한다고 가정하자. 

  우리는 이 확률 분포가 무엇인지 모르지만, 그러한 확률 분포를 최대한 비슷하게 찾아내려고 한다. 
  
  그러면 우리가 할 수 있는 것은, 데이터들의 집합 중에서 어떠한 데이터를 추출하였을 때, 그 데이터가 이 확률분포로부터 나왔을 확률을 계산하는 것이다. 

  즉, **가능도**란 어떤 확률분포로부터 이 값이 나올 확률인 것이다. 즉 쉽게 말해서 이 데이터가 이 확률분포를 따를 확률을 의미한다. 
  
  그렇다면 우리는 **고정된** 확률분포로부터 데이터가 나올 확률을 어떻게 알 수 있을까?
  
  바로 **확률분포를 가정하는 것**이다!!!

  그리고 데이터들을 가설로 세운 확률분포에 대입하여 **나오는 확률을 가능도**라고 할 수 있다.

  사실 이산적인 데이터들에서 가능도 = 확률로 봐도 되지만, 연속적인 경우에는 가능도 != 확률이다. 

  (PDF에서 y축의 값이 확률이 아닌 것처럼) 

  > 내가 이해한 바로는 random variable이란 표본 공간에서 특정 확률로 발생할 수 있는 event들을 특정한 값으로 mapping시켜주는 함수라고 한다. 
  >
  > 즉, 예를 들어 주사위에서 표본 공간에는 총 여섯개의 사건들이 있으며, 이러한 event들 중에서 주사위 눈금 1이라는 사건이 나올 확률을 P(X=1)이라고 표현한다고 이해했다. 
  >
  > 가능도 관점에서 봤을 때, 어떠한 확률분포를 가정하고, 주어진 event들을 대입하였을 때 나오는 확률들이  가능도라고 한다면, 이것은 결국 원래 확률분포에 데이터를 대입하는 것과 같다고 생각한다. 
  >
  > 하지만, 가능도라고 얘기하는 이유는 만약 수많은 데이터 샘플들이 존재한다고 했을 때, 원래 확률분포에서 이 데이터들의 확률의 합은 1에 가까워야 한다. 
  >
  > 만약 가정한 확률분포에서 theta값을 조정하면서 이 데이터들을 넣었을 때, 확률 값이 1에 가까워야 원래 확률분포와 비슷하다고 생각할 수 있는 것 같다. 
  >
  > 그러면, 만약 샘플들의 수가 충분하지 않을 경우에는 원래 분포를 잘 추정하지 못하는 것은 아닌가
  
- **최대가능도 추정법 (MLE, maximum likelihood estimation)**

  표본평균이나 표본분산은 중요한 통계량이지만, 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 된다. 

  이때, 최대가능도 추정법은 이론적으로 가장 가능성이 높은 모수를 추정한다.   
  ![image](https://user-images.githubusercontent.com/71866756/150474054-8f60ba9d-0e5c-47b3-a4b2-a99fbf463fff.png)  
  

  - **가능도 함수**  
    ![image](https://user-images.githubusercontent.com/71866756/150474153-5e6a32f3-badf-4460-b2a3-1b27458a6f5e.png)  
    데이터 집합 X가 **독립적**으로 추출되었을 경우 **로그가능도를 최적화**한다.   
    ![image](https://user-images.githubusercontent.com/71866756/150474188-943d763c-17c0-41a3-a9c4-e549ff8f6dbb.png)  
    log함수의 특성상 곱을 합으로 바꿔준다. 이러한 특성을 이용해서 곱으로 표현되어 있던 likelihood함수를 summation으로 바꿔준다. 

  - **로그가능도를 사용하는 이유**

    1. 데이터의 수가 수억 단위이면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능하다.

       (연산 오차 때문에 곱셈에서 오차가 발생)

    2. 경사하강법으로 가능도를 **최적화**할 때 미분 연산을 사용하는데, 로그 가능도를 사용하면 연산량을 O(n^2)에서 O(n)으로 줄일 수 있다. 

       (`최적화` : loss function의 결과값을 최소화하는 모델의 인자를 찾는 것을 의미)

    3. 로그가능도의 경우 보통 maximum을 찾는데 사용하기 때문에, 대게의 손실함수에서 사용하는 경사하강법을 이용하려면 **음의 로그가능도** (negetive log likelihood)를 이용하여 최적화한다. 

**EX) 최대가능도 추정법 예제 : 정규분포**

정규분포를 따르는 확률변수 X로부터 독립적인 표본 {x1, ..., xn}을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하라  
![image](https://user-images.githubusercontent.com/71866756/150474232-1b2fdbc5-1553-4fc4-9b22-55acf761e980.png)  
여기서 argmax_theta 즉, theta에 대해서 가장 큰 값을 찾아야 함으로 미분을 취해서 0인 값을 찾으면 된다. (2차함수에서 극대, 극소 찾듯이)   
![image](https://user-images.githubusercontent.com/71866756/150474273-b657c164-d6fc-44c9-a51d-3188840a8041.png)  
(MLE는 unbiased 추정량을 보장하진 않는다!)

**EX1) 최대가능도 추정법 예제 : 카테고리 분포 (Multinoulli)**

카테고리 분포 Multinoulli(x;p1,...,pd)를 따르는 확률변수 X로부터 독립적인 표본 {x1,...,xn}을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하라

카테고리 분포의 모수 p1부터 pd는 1차원부터 d차원까지 각각의 차원에서 값이 1 또는 0이 될 확률로, 이 확률을 모두 더하면 1이다. 

(제약식이 있는 경우 라그랑주 승수법을 사용)  
![image](https://user-images.githubusercontent.com/71866756/150474316-09404dc2-b0de-477c-9096-8ef4c56f8965.png)  

- **딥러닝에서 최대가능도 추정법**  
  ![image](https://user-images.githubusercontent.com/71866756/150474353-40afb9b0-6b90-49a5-ad8c-d8b70b0cd6d3.png)  
  
- **확률분포의 거리 구하기**

  loss function들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도된다. 

  데이터공간에 두개의 확률분포 P(x), Q(x)가 있을 경우 두 확률분포 사이의 거리(distance)를 계산할 때 다음과 같은 함수들을 사용한다. 

  - 총변동 거리 (TV, Total Variation Distance)

  - **쿨백-라이블러 발산 (KL, Kullback-Leibler Divergence)**  
   ![image](https://user-images.githubusercontent.com/71866756/150474394-ce916bf7-7814-4be3-a284-76b84a2c6f94.png)  
    왼쪽 식이 이산, 오른쪽 식이 연속인 경우이며, 위 식은 아래와 같이 분해될 수 있다.   
    ![image](https://user-images.githubusercontent.com/71866756/150474457-c868e695-b18c-4223-b8d4-2b703d18dcc8.png)  

    분류문제에서 정답레이블을 P, 모델 예측을 Q라 두면 **최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것**과 같다. 

    쉽게 말해서, 쿨백-라이블러 발산을 통해 확률분포의 거리를 구하는 것과 최대가능도 추정법에서의 로그가능도를 최대화하는 것은 밀접한 관련이 있다는 뜻이다. 

    두개의 확률분포의 거리를 최소화시킨다는 것은 주어진 데이터를 통해서 목적으로 하는 확률분포에 최적화된 모수를 구하는 것과 동일한 개념이다.

  - 바슈타인 거리 (Wasserstein Distance)
