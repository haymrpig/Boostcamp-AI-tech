# **Multi-GPU 학습**

오늘날의 딥러닝은 엄청난 양의 데이터 다룬다. 이러한 양의 데이터를 처리하기 위해서는 얼마나 많은  GPU를 확보했냐가 중요하다. 

AWS, GCP, naver cloud를 이용해서 Multi-GPU를 확보할 수 있다. 

- GPU vs Node

  node는 하나의 컴퓨터 (system)라고 생각

  - single node single gpu

  - single node multi gpu (우리는 이 경우를 주로 다룬다)

  - multi node multi gpu (서버실을 생각하면 쉽다, resource도 많이 들고 어렵다)

- NVIDIA에서는 TensorRT를 지원하여 GPU처리를 도와준다. 

# Model parallel

- 다중 GPU에 학습을 분산하는 두가지 방법

  - 모델 나누기 

    AlexNet이 모델을 나누는 대표적인 예시로 예전부터 사용했었다. 

  - 데이터 나누기

    데이터 나누기는 데이터를 여러개의 GPU에 나누고, 각각의 loss의 평균을 학습에 이용하는 방법

- 모델의 병목, 파이프라인의 어려움 등으로 인해 모델 병렬화는 고난이도 과제이다.

  난이도가 높아 흔하게 많이 쓰진 않는다.

  두개의 GPU에 모델을 나눴다 하더라도 제대로 구현되지 않으면 하나의 GPU의 작업이 끝나고 두번째 GPU의 작업을 시작하는 등 전혀 효율이 높아지지 않게 된다. 

  `파이프라인` : machine learning 과정에서의 workflow라고 생각하면 된다. 

  ![image-20220127102847067](C:\Users\Administrator1\AppData\Roaming\Typora\typora-user-images\image-20220127102847067.png)

  ```python
  # 이런식으로 할 경우, 병렬화가 잘 되지 않기 때문에 이렇게 코드를 작성하진 않는다. 
  class ModelParalleResNet(ResNet):
  	def __init__(self, *args, **kwargs):
  		super().__init__(Bottleneck, [3,4,6,3], num_classes=num_classes, *args, **kwargs)
  		
  		self.seq1 = nn.Sequanial().to('cuda:0')
  		self.seq2 = nn.Sequential().to('cuda:1')
  		self.fc.to('cuda:1')
  		# 위 예시처럼 서로 다른 GPU에 layer를 할당한다. 
          
  	def forward(self, x):
  		x = self.seq2(self.seq1(x).to('cuda:1'))
          # GPU0에서 나온 output을 GPU1로 copy해서 보낸다. 
  		return self.fc(x.view(x.size(0), -1))
      
  ```

  

# Data parallel

- 데이터를 나눠 GPU에 할당 후 결과의 평균을 취한다.

  (minibatch도 비슷한 방식이라고 볼 수도 있다.)

- 과정

  - 마찬가지로 Data parallel도 Data를 나누고, 각각의 GPU에 모델에 데이터를 전송하여 돌리고, 하나의 GPU에 모든 output을 모아서 loss를 평균낸다. 

    (global interpreter lock 문제가 있을 수 있다)

    그 다음, 각각의 gradient를 계산하여 다시 여러개의 GPU에 gradient값을 넘겨주고, 다시 마지막에 하나의 GPU에 backward로 계산한 gradient를 전송하고 평균낸다. 


  ![img](https://miro.medium.com/max/2000/1*FpDHkWJhkLL7KxU01Lf9Lw.png)

- PyTorch에서는 두가지 방식 제공

  DataParallel, DistributedDataParallel

- Dataparallel : 단순히 데이터를 분배한 후 평균을 취함

  - GPU 사용 불균형 문제 발생 ( 하나의 GPU에 다 모아서 처리하니깐 )

  - Batch 사이즈 감소( 하나의 GPU가 더 많은 작업을 하니깐 병목현상 발생)

  - Python GIL(global interpreter lock)

    단순히 멀티 스레드를 이용하는 경우 GIL 때문에 오히려 성능이 저하될 수 있다. 

    GIL은 파이썬 인터프리터가 한 스레드만 하나의 바이트코드를 실행 시킬 수 있도록 해주는 lock이다. 

    즉 멀티 스레드라 하더라고, 하나의 스레드에 모든 자원을 허락하고 lock을 걸어 다른 스레드는 실행할 수 없게 막는 것이다. 따라서 스레드지만, lock이 걸려 마치 sequential하게 동작하는 것이며, thread context switch의 비용까지 더해지면 오히려 느려지는 것이 당연하다. 

    lock을 거는 이유는 하나의 객체를 여러 스레드가 공유할 때, reference count를 관리하기 위해서이다. ( 쉽게 말하면 하나의 객체에 여러 스레드가 접근해서 값을 막 바꾸는 걸 말하는 것 같다.)

    - 그럼 왜 멀티 스레드를 사용하는가?

      작업 중간중간 I/O 작업이나 sleep 함수가 있어 대기해야 하는 경우에 context switching이 일어나서 다른 스레드에서 작업을 하기 때문에 성능이 좋아질 수 있다. 

```python
parallel_model = torch.nn.DataParallel(model) # Encapsulate the model

predictions = parallel_model(inputs)          # Forward pass on multi-GPUs
loss = loss_function(predictions, labels)     # Compute loss function
loss.mean().backward()                        # Average GPU-losses + backward pass
optimizer.step()                              # Optimizer step
predictions = parallel_model(inputs)          # Forward pass with new parameters
```



- [DistributedDataParallel](https://bit.ly/37usURV) : 각 CPU마다 process를 생성하여 개별 GPU에 할당
  - 기본적으로 Dataparallel로 하나 개별적으로 연산의 평균을 낸다. 

```python
train_sampler = torch.utils.data.distributed.DistributedSample(train_data)
shuffle = False
pin_memory = True
# 메모리에 데이터를 바로바로 올릴 수 있도록 절차를 간소화 시킨것

trainloader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=shuffle, pin_memory=pin_memory, num_workers=3, sampler=train_sampler)
```

- [**python 멀티프로세싱 코드**]([Multi GPU with Pytorch (DistributedDataParallel) (si-analytics.ai)](https://blog.si-analytics.ai/12))

```python
def main():
	n_gpus = torch.cuda.device_count()
	torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, ))

def main_worker(gpu, n_gpus):
    image_size = 224
    batch_size = 512
    num_worker = 8
    epochs = ...
    batch_size = int(batch_size / n_gpus)
    num_worker = int(num_worker / n_gpus)
    
    # 멀티프로세싱 통신 규약 정의 (각각의 프로세스들이 데이터를 주고받아야 하므로)
    torch.distributed.init_process_group(
    									backend='nccl’, 																init_method='tcp://127.0.0.1:2568’, 											world_size=n_gpus, 																rank=gpu)
    
    model = MODEL
    torch.cuda.set_device(gpu)
    model = model.cuda(gpu)
    
    # Distributed dataparallel 정의
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])
	"""
	train 코드 추가
	"""
# 멀티프로세싱 코드
from multiprocessing import Pool

def f(x):
	return x*x
	
if __name__=='__main__':
	with Pool(5) as p:
		print(p.map(f, [1,2,3]))
        
    
```



# 질문하기

같은 성능의 GPU가 여러장 있다고 할 때, 취합할 때의 GPU가 더 많은 작업을 해야하기 때문에, Batch 사이즈가 감소한다고 했는데, 어차피 loss랑 gradient만 마지막에 전송하는 건데 메모리가 부족할 수가 있나? (batch size를 조절하는게 메모리가 부족해서 그런게 아닌가?)

DistributedDataParallel -> 또 다른 Data parallel 방법으로 하나의 GPU에 모으는 것이 아닌 각각의 GPU에 그대로 두고 한다는데, 이건 그냥 데이터 조금씩 여러개의 컴퓨터에서 돌리는것과 뭐가 다름? -> 개별적으로 연산의 평균을 낸다는게 이해가 안됨