# PyTroch Trouble shooting

### OOM (out of  memory)

- 발생 원인을 알기 어려우면, 발생한 곳을 알기도 어렵다.
- Error backtracking이 이상한 곳으로 가며, 메모리의 이전상황의 파악이 어렵다. 

- 단순한 해결방법
  - Batch size를 줄인다.
  - GPU를 clean한다. 
  - colab의 경우 다시 launch한다. 

### [GPUUtil](https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/)

- nvidia-smi처럼 GPU의 상태를 보여주는 모듈 (nvidia와 달리 매 iteration마다 확인이 가능하다)

```python
!pip install GPUtil

import torch
import GPUtil
from GPUtil import showUtilization as gpu_usage

gpu_usage()

tensorList=[]
for x in range(10):
    tensorList.append(torch.randn(100000000, 10).cuda())
# 메모리 사용하는 중을 가정
# 메모리가 계속 늘어나면 잘못된 것, 메모리가 쌓이고 있는 것

gpu_usage()

del tensorList
# 메모리를 비워줫지만 garbage collector(GPU 내부의 남는 메모리를 긁어온다.)가 작동하지 않은 상태라
# 실제로 비운 메모리를 사용할 수 없다. 
# gpu_usage()를 통해 확인해보면 그대로 메모리가 차 있는 것을 확인할 수 있다. 
gpu_usage()

torch.cuda.empty_cache()
# garbage collector를 통해 메모리를 확보할 수 있음
gpu_usage()
```



- **torch.cuda.empty_cache()**

  - 사용되지 않은 GPU상 cache를 정리, 가용 메모리 확보

  - del과는 구분이 필요하며 reset 대신 쓰기 좋은 함수이다. 

    (del은 관계를 끊으면서 메모리를 free하는 것)




<<<<<<< HEAD
### [Training loop에 tensor로 축적되는 변수 확인하기](https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/)
=======
### [Training loop에 tensor로 축적되는 변수 확인하기]([Memory Management, Optimisation and Debugging with PyTorch (paperspace.com)](https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/)
>>>>>>> 4340e716df912b945ce83cc71a5fe640f88256b4

- tensor로 처리된 변수는 GPU 상의 메모리를 사용한다.

- 해당 변수 loop 안에 연산에 있을 때, GPU에 computational graph를 생성하기 때문에, 해당 변수에 새로 값을 할당한다 할지라도 사라지지 않고 메모리 상에 남게 된다. 

  (메모리 잠식)

- 보통은 backward()가 위에서 실행이 되어있으면, graph는 free된다. 

```python
total_loss = 0
for i in range(10000):
	optimizer.zero_grad()
	output = model(input)
	loss = criterion(output)
	loss.backward()
	optimizer.step()
	total_loss += loss
```

- 위 예시같은 경우에, 구한 loss들을 저장해두어 backward propagation을 진행한다. 

- total_loss += loss를 진행할 때, tensor이기 때문에 graph를 만들어 버린다. 

  ![image](https://user-images.githubusercontent.com/71866756/151500466-533cca88-3e3a-4f16-963d-3d11cc34d9a3.png)

- 위 그림처럼 graph는 사라지지 않기 때문에, 이를 해결해주기 위해서 아래 코드처럼 작성한다. 

```python
total_loss = 0
for x in range(10):
	# assume loss is computed
    iter_loss = torch.randn(3,4).mean()
    iter_loss.requires_grad = True
    total_loss += iter_loss.item()
    # .item을 통해 1-d tensor의 경우는 python 기본 객체로 변환하여 처리한다. 
```

- .item()은 python 데이터 타입을 반환하기 때문에, graph를 생성하지 않고 메모리를 자동으로 free할 수 있게 된다. 



### del명령어 적절히 사용하기

- python의 메모리 배치 특성상 loop가 끝나도 메모리를 차지하기 때문에 필요가 없어진 변수는 적절한 삭제가 필요하다.

  (c++의 경우 for(int i=0;i<1000;i++)에서 i는 for 루프 안에서만 존재하며, 밖에서는 free된다)



### 가능한 batch 사이즈 실험해보기

- OOM이 발생한다면 batch사이즈를 1로 실험해본다. 

  ```python
  oom=False
  try:
  	run_model(batch_size)
  except RuntimeError:
  	oom = True
  	
  if oom:
  	for _ in range(batch_size):
  		run_model(1)
  ```

- OOM error가 발생할 경우 위 코드처럼 batch_size를 1로하여 실험해본다. 

- 원래 batch_size가 32였을 경우, batch는 1로 32번 검사해보면 문제의 원인이 batch_size였는지 확인해볼 수 있다. 



### torch.no_grad() 사용하기

- inference 시점에서는 torch.no_grad() 구문을 사용하기
- backward pass로 인해 쌓이는 메모리에서 자유롭다. 

```python
with torch.no_grad():
    for data, target in test_loader:
        output = network(data)
        test_loss += F.nll_loss(output, target, size_average=False).item()
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).sum()
```



### [예상치 못한 에러 메세지](https://brstar96.github.io/shoveling/device_error_summary/)

- CUDNN_STATUS_NOT_INIT 이나 device-side-assert 등이 발생할 수 있다. 
- 에러 발생 시 관련 내용을 찾아보고, 해결했다면 기록해두는 것도 좋은 방법이다. 



### 그 외

- colab에서 너무 큰 사이즈는 실행하지 말 것

- CNN의 대부분의 에러는 크기가 맞지 않아 생긴다.

  (torchsummary 등으로 사이즈를 맞출 것)

- tensor의 float precision을 16bit로 줄일 수도 있다. 

  (보통 32bit를 쓰고, 16bit으로 줄이는 경우는 작은 모델에서는 잘 쓰지 않는다)
