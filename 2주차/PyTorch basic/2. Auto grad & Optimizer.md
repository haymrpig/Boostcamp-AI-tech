# Auto grad & Optimizer

### 개요

대부분의 논문에는 모델에 대한 상세한 정보와 그림이 주어지며, 이러한 모델들은 상당 부분 layer(block)의 반복으로 구성되어 있다. 

block의 output을 다음 block의 input으로 넣으면서 back propagation을 통해 gradient를 계산하고, 이 gradient를 통해 parameter를 업데이트한다. 

모델에서 `encoding layer`란 암호화하는 layer, `decoding layer`란 해독하는 layer로 computer vision분야를 예로 생각해보면 input 이미지가 encoding layer를 통해 암호화되고, decoding layer를 통해 암호를 해독하여 우리가 원하는 결과를 낼 수 있게 학습한다고 생각할 수 있다. 

### torch.nn.Module

- 딥러닝 모델을 구성하는 layer의 base class

- input, output, forward, backward를 정의한 class이다. 

  ( input, output : optional이지만 보통 정의, forward : process 또는 formula, backward : Auto grad, weight를 갱신)

- 학습의 대상이 되는 parameter (tensor)를 정의한다.



### nn.Parameter

- 상대적으로 low level이라 이걸 직접 사용하는 경우는 거의 없다.

- weight를 정의하며 Tensor 객체의 상속 객체이다. 

- nn.Module 내에서 attribute가 될 때는 required_grad=True로 지정되어 학습의 대상이 된다.

  ( 자동미분의 대상이 되며, required_grad는 직접 설정하는 경우는 거의 없다. ) 

- parameter를 그냥 tensor로 지정한 경우 Auto grad가 되지 않는다!!



### Backward

- layer의 parameter들의 미분을 수행한다.

- forward의 결과값(output)과 정답 label간의 차이(loss)에 대해 미분을 수행하며, 미분의 결과값으로 parameter를 업데이트 한다. 

  (backward함수가 호출되면 자동미분 진행)

- 실제 하드코딩을 통해 backward를 module 단계에서 직접 지정이 가능하지만, Auto grad를 이용하기 때문에 자주 보이진 않는다. 

  ( 직접 지정을 하기 위해서 Module에서 backward와 optimizer를 오버라이딩하여 사용한다. )



### 질문사항

- 1 epoch에서 이뤄지는 모델 학습 과정을 정리해보고, 성능을 올리기 위해서 어떤 부분을 고려해야 하는가?

  1. 먼저 optimizer와 loss function을 지정한다. 

  2. gradient를 0으로 초기화한다. 

  3. training data를 모델에 넣어 output과 정답 label의 차이 (loss)를 loss function으로 구한다.
  4. Auto grad를 통해 gradient를 계산한다.
  5. 계산한 gradient를 통해 parameter를 업데이트한다. 

  성능을 높이기 위해서는 다양한 방법을 사용한다. data augmentation, minibatch 이용, 적절한 learning rate 조절, 적합한 loss function과  모델 사용 등등

- optimizer.zero_grad()를 하지 않으면 어떤 일이 일어날지 그리고 매 batch step마다 항상 필요한가?

  PyTorch는 gradient를 계속해서 accumulate하는 특징을 가지고 있다. ( gradient를 계속해서 더해주는데, RNN에 유리하게 하기 위해서인 듯하다. )

  따라서 gradient를 초기화하지 않을 경우, 이전 step에서의 gradient와 현재 step에서의 gradient가 서로 더해져 학습에 방해가 될 수 있다. 

  [**Ref**](https://yeko90.tistory.com/entry/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-%EA%B8%B0%EC%B4%88-modelzerograd-optimizerzerograd-%EC%93%B0%EB%8A%94-%EC%9D%B4%EC%9C%A0)

